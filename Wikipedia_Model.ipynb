{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import operator\n",
    "import time\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from __future__ import unicode_literals, print_function\n",
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Article\n",
    "\n",
    "One can assign any string to doc1 to analyze the article. Simple function which will have an input for document will follow in the next update of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc1=\"\"\"\n",
    "At the start of the last ice age, 2.6 million years ago, a sheet of frozen water formed atop North America that kept expanding and thickening until it reached a maximum depth of roughly two miles.   \n",
    "At its southern edge, the vast body deposited tons of rocky debris — from sand and pebbles to boulders the size of school buses. Then, some 18,000 years ago, the planet began to warm and the gargantuan sheet of ice began to melt and retreat.\n",
    "\n",
    "Today, the southernmost edge of that frozen expanse is marked by a line of rubble that extends across the northern United States for thousands of miles. The largest deposits form what geologists call a terminal moraine.\n",
    "\n",
    "The intermittent ridge runs from Puget Sound to the Missouri River to Montauk Point on Long Island, forming the prominence that supports its old lighthouse. The ancient sheet of ice also left its mark on a very modern phenomenon: New York City.\n",
    "The ice over Manhattan would have buried even the tallest skyscraper and was so heavy that it depressed the underlying bedrock. As it melted, giant boulders embedded deep within its flanks landed throughout what became the city. Many are still visible in Central Park, unlikely obelisks scored by time.\n",
    "\n",
    "But the island was the last hurrah, and the mammoth sheet of ice ended immediately to the south, in Brooklyn, Queens and Staten Island. The terminal moraine, the mounds of rubble left behind, form much of their high ground.\n",
    "While the line of glacial debris across the northern United States is often poorly delineated, the hilly ridge around New York City tends to be quite prominent. Its maximum height is roughly 200 feet, about that of a tall apartment building.\n",
    "\n",
    "The rubble slowed the development of the other boroughs for centuries. Early developers sought flat land for homes and buildings and typically ignored the glacial ridges, especially their heights. Land there was too inaccessible, stony and yielding for easy construction.\n",
    "Eventually, the neglected parcels became strings of parks, cemeteries, golf courses and, in time, some of the region’s most attractive neighborhoods, often heavily landscaped and densely wooded.\n",
    "Today the ridgeline and its adjoining slopes hold Forest Park and Highland Park in Queens, as well as Prospect Park in Brooklyn. The nearby grounds of Green-Wood Cemetery include Battle Hill, a part of the glacial rise that represents the borough’s highest point.\n",
    "\n",
    "Many city neighborhoods take their names from the ridge’s elevations, as well as its leafy embellishments: Glen Oaks, Hollis Hills, Jamaica Hills, Briarwood, Forest Hills, Ridgewood, Cypress Hills, Crown Heights, Prospect Heights, Boerum Hill, Cobble Hill, Park Slope, Greenwood, Bay Ridge, Lighthouse Hill and Arden Heights.\n",
    "\n",
    "Why should remnants of the last ice age be so prominent in New York City, of all places?\n",
    "It started long ago as the continental ice sheet developed one of its thickest regions over what is now eastern Canada. Vast fields of ice are incredibly weighty. Gravity pushes down hard. Any tilt or slant of the underlying terrain sends the ice moving slowly downslope.\n",
    "\n",
    "For ages, the Canadian ice kept flowing into a long valley to the south that ran from the Lake Champlain area down through the Hudson River gorge. It was a reliable conduit. The river of ice and rock kept moving relentlessly southward until it hit a region warm enough to halt its slow advance.\n",
    "\n",
    "During ice ages, glaciers advance and retreat in cycles. Paths differ. But overall, the southern end of the conduit saw massive buildups of boulders and rubble.  \n",
    "In recent times, the resulting moraine got much scrutiny from scientists because of its proximity to one of the nation’s first big cities. Just as early maps of the United States often focused on New York, so did geologic inquiries — starting nearly two centuries ago.\n",
    "\n",
    "Despite the ridge’s prominence and early allure for scientists, it turned out to be no rival for skyscrapers and urban distractions. The moraine that shaped the city was all but forgotten.\n",
    "\n",
    "“Clearly, it’s not on the radar,” said David E. Seidemann, a professor of geology at Brooklyn College. “The educational system here doesn’t emphasize earth science. And there’s so much else to do. I’ll go to a Yankees game over geology any day.”\n",
    "Springtime, noted Sidney Horenstein, a geologist and environmental educator emeritus at the American Museum of Natural History, turns the glacial rise into a band of leafy green and opens a season favorable to its rediscovery.\n",
    "\n",
    "“It makes it much easier to see,” he said recently, while gazing at the ridge from the Staten Island Ferry.\n",
    "Mr. Horenstein, 81, sought to deepen the re-examination by sharing with The Times a vast trove he had assembled of scientific studies, official reports, news clippings, magazine articles and old books that mention or profile the city’s glacial relic.\n",
    "His collection also sheds light on how moraine study in general helped geologists in New York and elsewhere discover that the planet had experienced a series of ice ages. That breakthrough came in the mid 1800s.\n",
    "\n",
    "Ridges, mountains and even flatlands are typically rooted in rocky strata, such as the bedrock that underlies Manhattan and makes it ideal for erecting skyscrapers. But early investigators found the hilly ridges to be composed of clay, silt, sand, pebbles, cobbles and boulders, all jumbled up together.\n",
    "\n",
    "A related clue was that nearby rocks often looked quite smooth, even polished. At times, their faces also displayed parallel lines of groves and striations.\n",
    "\n",
    "In 1843, an official report on the natural history of New York State cited vast glaciers as a possible explanation “now attracting much attention” for what geologists were uncovering. But it also listed a dozen other theories, including “the deluge of Noah,” or the biblical flood.\n",
    "\n",
    "By the 1860s, a growing body of evidence had convinced most scientists that the Earth had endured ages in which rivers of ice transported rocks and coarse sediments over long distances — at times, it turned out, for hundreds of miles.\n",
    "\n",
    "In the 1880s, the term “ice age” was coming into wide use, and experts began looking into some of the practical consequences.\n",
    "Then in 1902, the United States Geological Survey published a large folio on metropolitan New York that detailed its rocky underpinnings, including the ridge. Multicolored maps showed the glacial rise amid emerging street grids and neighborhoods.\n",
    "\n",
    "The accompanying report said the rise exhibited strings of “hillocks and hollows, or interrupted ridges and troughs.” It noted that some depressions held ponds, marshes and small lakes. The report put the feature’s overall width at up to two miles.\n",
    "At first, the city used the stony ridge for woodlots and rain catchments. Slowly, the uses expanded to reservoirs, recreational areas and, in time, neighborhoods in which buildings and houses were built on strong footings and foundations for stability.\n",
    "\n",
    "Today, despite the wide development of the ridge’s lower slopes, a Google Earth view of New York City — a composite of images from April, June and September — shows the glacial relic as an intermittent band of green.\n",
    "Talkative and outgoing, his shirt often untucked, the model of a rumpled geologist, Mr. Horenstein is a native New Yorker with boyish enthusiasm for the city’s hidden faults and early beginnings, for ancient blows and catastrophes. A compendium of geologic jokes, he refers to himself not as a raconteur but a rockconteur.\n",
    "\n",
    "Though long retired, Mr. Horenstein regularly gives public tours of the city’s geology, some of them organized by the museum. Recently, at a reporter’s invitation, he turned his attention to the glacial ridge.\n",
    "\n",
    "At Umpire Rock in Central Park, overlooking baseball fields, the geologist noted places where glacial ice and rubble had carved massive grooves, wider than a human body.\n",
    "\n",
    "Yet the rock’s overall surface was quite smooth. The reason, Mr. Horenstein said, was that ages of glacial abrasion had acted like sandpaper.\n",
    "\n",
    "“Kids can slide down the rocks,” he said of many Central Park outcroppings.\n",
    "Throughout the park are places where the retreating ice had dropped giant boulders that geologists call erratics, after the Latin word errare, to wander. By definition, the boulders differ in composition from surrounding rocks. Some of the park’s erratics sit perched atop flat rocks, looking at times like alien monuments.\n",
    "“They’re rounded,” Mr. Horenstein said, because rivers of ice kept “dragging them around.”\n",
    "\n",
    "Later, on a visit to Staten Island, he pointed to the much-celebrated skyline of Lower Manhattan. Current estimates put the ice’s thickness there at roughly twice the early calculations — not 1,000 feet, but closer to 2,000 feet, and possibly more.\n",
    "\n",
    "“It was taller than any building,” Mr. Horenstein remarked, “even the Freedom Tower.” The skyscraper at the very top of its spire measures 1,776 feet high, after the nation’s birthday.\n",
    "\n",
    "The forward edges of glaciers can be inclined or sheer, like a cliff. Mr. Horenstein said geologists believe the local face was sheer. Its precipitous edge shed not only tons of rocky debris but gargantuan blocks of ice.\n",
    "\n",
    "The ice’s overall weight was so immense that it depressed the bedrock of the New York City region — and then, following the sheet’s retreat, the rocky depths slowly rebounded. Mr. Horenstein said the rise is calculated at more than 150 feet.\n",
    "At Fort Wadsworth, a historic battlement next to the Verrazano Bridge, we stood atop the moraine and looked across the Narrows, the strait between Staten Island and Brooklyn. The land that would become the two boroughs had originally been connected by the glacial ridge.\n",
    "\n",
    "Some 13,000 years ago, a large accumulation of icy water from melting glaciers was suddenly unleashed upstate. A towering wave of destruction crashed down through the Hudson gorge and proceeded to smash the southern end of the local moraine to smithereens.\n",
    "“It was biblical,” Mr. Horenstein said. The wave created the Narrows, which now connects the Atlantic Ocean to one of the world’s largest natural harbors.\n",
    "\n",
    "In the metropolitan area, the southern tip of Staten Island overlooks the Arthur Kill and Raritan Bay, and represents the southernmost part of the glacial rise. The site hosts the Conference House Park. It takes its name from an unsuccessful peace conference held there in 1776 during the Revolutionary War.\n",
    "\n",
    "As we walked past a stone manor house where the meeting took place, Mr. Horenstein pointed to the rounded rocks in its walls and, based on color and texture, proceeded to tick off their likely sites of origin —  some from upstate New York, some from New Jersey, and so on.\n",
    "\n",
    "What fraction came from the glacial ridge?\n",
    "\n",
    "“All of them,” he replied. “It was the local building material.”\n",
    "\n",
    "We walked down to the nearby beach and wandered a few hundred feet to where the land formed a sandy bluff. It offered a rare glimpse into the heart of the ridge: a jumble of clay, silt, pebbles and boulders in a fragile matrix laid bare by the action of tides, hurricanes and pounding waves.\n",
    "\n",
    "In all of New York City, “this is the only place you can see the moraine clearly,” Mr. Horenstein said, leaning into the wind. “This is it, the southernmost end.”\n",
    "\n",
    "Mr. Horenstein held up a rounded stone. Somehow, it ended up in reporter’s backpack as a souvenir of the day.\n",
    "Remarkably, millions of people live on or near the glacial ridge.\n",
    "\n",
    "In all, it runs for roughly 30 miles beneath New York City. Invisibly, it links three boroughs, offering mute testimony to the power of vanished ice.\n",
    "\n",
    "If the ridge is lost history to most of the city’s inhabitants, at least one knows something about the art of bringing it back to life.\n",
    "\n",
    "“It keeps me young,” Mr. Horenstein said. “There’s always something to see, something you missed, something new.”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#Needing a better preprocessor for words like U.S.\n",
    "def preprocessor(text):\n",
    "    text=re.sub('[\\W]+',' ', text.lower()) \n",
    "    return text\n",
    "doc1lower=preprocessor(doc1)\n",
    "\n",
    "\n",
    "def preprocessor2(text):\n",
    "    text=re.sub('[\\W]+',' ',text) \n",
    "    return text\n",
    "doc1upper=preprocessor2(doc1)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "newStopWords = ['the','would']\n",
    "stop.extend(newStopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capital Lettered Word List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "capitalletters=nltk.word_tokenize(doc1)\n",
    "#capitalletters=doc1upper.split()\n",
    "capital=list()\n",
    "for i in capitalletters:\n",
    "    if i[0].isupper():\n",
    "        capital.append(i)\n",
    "        \n",
    "for i in range(len(capital)):\n",
    "    capital[i]=capital[i].lower()\n",
    "\n",
    "for i in capital[:]:\n",
    "    if i in stop or len(i)<=1:\n",
    "        capital.remove(i)\n",
    "\n",
    "capitaloccurence=list()\n",
    "for i in range(len(capital)):\n",
    "    capitaloccurence.append(capital.count(capital[i]))\n",
    "    \n",
    "capitaldic = dict()\n",
    "for j in range(len(capital)):\n",
    "    if not capital[j] in capitaldic.keys():\n",
    "        capitaldic[capital[j]] = capitaloccurence[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of tokens and removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splitted=nltk.word_tokenize(doc1.lower())\n",
    "#splitted=doc1lower.split()\n",
    "#print(splitted)\n",
    "for i in splitted[:]:\n",
    "    if i in stop or len(i)<=1:\n",
    "        splitted.remove(i)\n",
    "        \n",
    "numbers = range(len(splitted))\n",
    "#dictionary holding numbers and their texts\n",
    "holder = dict()\n",
    "\n",
    "for i in numbers:\n",
    "    if splitted[i] not in holder.keys():\n",
    "        holder[splitted[i]] = i\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "order=list() #order of my words according to their indexes\n",
    "for i in numbers:\n",
    "    if holder[splitted[i]] != i:\n",
    "        order.append(holder[splitted[i]])\n",
    "    else:\n",
    "        order.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2-gram pairs\n",
    "pairs=list()\n",
    "for i in range(len(order)):\n",
    "    try:\n",
    "        pairs.append([order[i],order[i+1]])\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "#3-gram pairs\n",
    "pairs3=list()\n",
    "for i in range(len(order)):\n",
    "    if i%2==0:\n",
    "        try:\n",
    "            pairs3.append([order[i],order[i+1],order[i+2]])\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Occurences for words, n-grams and capital lettered words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#of occurences for words\n",
    "occurence1=list()\n",
    "for i in range(len(order)):\n",
    "    occurence1.append(order.count(order[i]))\n",
    "\n",
    "#of occurences for 2-grams\n",
    "occurence=list()\n",
    "for i in range(len(pairs)):\n",
    "    occurence.append(pairs.count(pairs[i]))\n",
    "\n",
    "#of occurences for 3-grams\n",
    "occurence3=list()\n",
    "for i in range(len(pairs3)):\n",
    "    occurence3.append(pairs3.count(pairs3[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1-grams\n",
    "finaldic1 = dict()\n",
    "for j in range(len(order)):\n",
    "    if not order[j] in finaldic1.keys():\n",
    "        finaldic1[order[j]] = occurence1[j]\n",
    "        \n",
    "#2-grams\n",
    "finaldic = dict()\n",
    "for j in range(len(pairs)):\n",
    "    if not set(pairs[j]).issubset(finaldic.keys()):\n",
    "        finaldic[tuple(pairs[j])] = occurence[j]\n",
    "        \n",
    "#3-grams\n",
    "finaldic3 = dict()\n",
    "for j in range(len(pairs3)):\n",
    "    if not set(pairs3[j]).issubset(finaldic3.keys()):\n",
    "        finaldic3[tuple(pairs3[j])] = occurence3[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_of_words=4\n",
    "sorted_x1 = sorted(finaldic1.items(), key=operator.itemgetter(1))\n",
    "sorted_x = sorted(finaldic.items(), key=operator.itemgetter(1))\n",
    "sorted_x3 = sorted(finaldic3.items(), key=operator.itemgetter(1))\n",
    "sorted_capital = sorted(capitaldic.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Wikipedia Categories\n",
    "Removing unfrequent words first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in sorted_x1[:]:\n",
    "    if i[1]<2:\n",
    "        sorted_x1.remove(i)\n",
    "for i in sorted_x[:]:\n",
    "    if i[1]<2:\n",
    "        sorted_x.remove(i)\n",
    "for i in sorted_x3[:]:\n",
    "    if i[1]<2:\n",
    "        sorted_x3.remove(i)\n",
    "for i in sorted_capital[:]:\n",
    "    if i[1]<2:\n",
    "        sorted_capital.remove(i)\n",
    "sorted_xc = list()\n",
    "for i in range(len(sorted_capital)):\n",
    "    dummy=list()\n",
    "    dummy.append(holder[sorted_capital[i][0]])\n",
    "    dummy.append(sorted_capital[i][1])\n",
    "    sorted_xc.append(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onewordcategorylist=dict()\n",
    "categorydict=dict()\n",
    "twowordcategorylist=dict()\n",
    "capitalcategorylist=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from wiki import wikipediacategory\n",
    "from wiki1 import matrixcreation\n",
    "from wiki2 import wikipedialink\n",
    "from wiki3 import linkmatrixcreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikipediacategory(sorted_x1,onewordcategorylist,categorydict,sorted_x,twowordcategorylist,sorted_xc,capitalcategorylist,splitted,order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totallist = sorted_x1 + sorted_x + sorted_xc\n",
    "\n",
    "noofwords=list()\n",
    "for i in range(len(totallist)):\n",
    "    noofwords.append(totallist[i][0])\n",
    "noofwords = set(noofwords)\n",
    "noofwords=list(noofwords)\n",
    "\n",
    "Sorted_x = [item[0] for item in sorted_x]\n",
    "Sorted_x1 = [item[0] for item in sorted_x1]\n",
    "Sorted_xc = [item[0] for item in sorted_xc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "categorywordmatrix= matrixcreation(noofwords,Sorted_x1,onewordcategorylist,categorydict,splitted,Sorted_xc,capitalcategorylist,Sorted_x,twowordcategorylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = dict(zip(list(categorydict.values()),categorywordmatrix.sum(axis=0)))\n",
    "sortcat = sorted(dictionary.items(), key=operator.itemgetter(1))\n",
    "important_categories=list()\n",
    "for i in range(6):\n",
    "    for a,b in categorydict.items():\n",
    "        if b == sortcat[-1-i][0]:\n",
    "            #print (a)\n",
    "            important_categories.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Wikipedia Anchor Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onewordlinklist=dict()\n",
    "linkdict=dict()\n",
    "twowordlinklist=dict()\n",
    "capitallinklist=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikipedialink(sorted_x1,onewordlinklist,linkdict,sorted_x,twowordlinklist,sorted_xc,capitallinklist,splitted,order,doc1lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linkwordmatrix= linkmatrixcreation(noofwords,Sorted_x1,onewordlinklist,linkdict,splitted,Sorted_xc,capitallinklist,Sorted_x,twowordlinklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary2 = dict(zip(list(linkdict.values()),linkwordmatrix.sum(axis=0)))\n",
    "sortcat2 = sorted(dictionary2.items(), key=operator.itemgetter(1))\n",
    "important_links=list()\n",
    "for i in range(6):\n",
    "    for a,b in linkdict.items():\n",
    "        if b == sortcat2[-1-i][0]:\n",
    "            #print (a)\n",
    "            important_links.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_of_words=4\n",
    "important_grams=list()\n",
    "\n",
    "for i in range(no_of_words):\n",
    "    #print(splitted[order[sorted_x[-i-1][0][0]]],splitted[order[sorted_x[-i-1][0][1]]],\"\\n no of occurence:\",sorted_x[-i-1][1])\n",
    "    #print(\"Index number:\",sorted_x[-i-1][0][0],sorted_x[-i-1][0][1])\n",
    "    important_grams.append((splitted[order[sorted_x[-i-1][0][0]]]+\" \"+splitted[order[sorted_x[-i-1][0][1]]]))\n",
    "\n",
    "for i in range(no_of_words):\n",
    "    break_=0\n",
    "    #print(splitted[order[sorted_x1[-i-1][0]]],\"\\n no of occurence:\",sorted_x1[-i-1][1])\n",
    "    #print(\"Index number:\",sorted_x1[-i-1][0])\n",
    "    for j in important_grams:\n",
    "        if splitted[order[sorted_x1[-i-1][0]]] in j:\n",
    "            break_=1\n",
    "    if break_==0:\n",
    "        important_grams.append(splitted[order[sorted_x1[-i-1][0]]])\n",
    "\n",
    "for i in range(no_of_words):\n",
    "    break_=0\n",
    "    #print(sorted_capital[-i-1][0],\"\\n no of occurence:\",sorted_capital[-i-1][1])\n",
    "    for j in important_grams:\n",
    "        if sorted_capital[-i-1][0] in j:\n",
    "            break_=1\n",
    "    if break_==0:\n",
    "        important_grams.append(sorted_capital[-i-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_nlp=spacy.load('en')\n",
    "doc2 = en_nlp(doc1)\n",
    "sentences = [sent.string.strip() for sent in doc2.sents]\n",
    "\n",
    "for i in range(len(sentences[:])):\n",
    "    sentences[i]=preprocessor(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentencematrix=np.zeros(shape=(1,len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(sentences[:])):\n",
    "    sp=sentences[i].split()\n",
    "    for j in range(len(important_grams)):\n",
    "        if important_grams[j] in sentences[i]:\n",
    "            sentencematrix[0][i]+=1\n",
    "    for k in range(len(important_categories)):\n",
    "        for l in sp:\n",
    "            try:\n",
    "                sentencematrix[0][i]+=categorywordmatrix[holder[l]][categorydict[important_categories[k]]]\n",
    "                sentencematrix[0][i]+=linkwordmatrix[holder[l]][linkdict[important_links[k]]]\n",
    "            except:\n",
    "                None\n",
    "    #if len(sp)>0:\n",
    "    #    sentencematrix[0][i]/=len(sp)      \n",
    "    #else:\n",
    "    #    sentencematrix[0][i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "act_sentences = [sent.string.strip() for sent in doc2.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "important_sentences=list()\n",
    "important_sentences.append(act_sentences[np.argmax(sentencematrix[0])])\n",
    "sentencematrix[0][np.argmax(sentencematrix[0])]=-1\n",
    "important_sentences.append(act_sentences[np.argmax(sentencematrix[0])])\n",
    "sentencematrix[0][np.argmax(sentencematrix[0])]=-1\n",
    "important_sentences.append(act_sentences[np.argmax(sentencematrix[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Topic Model**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**General Categories:**  ['fluvial landforms', 'slope landforms', 'boroughs of new york city', 'glaciology', 'coastal and oceanic landforms', 'geomorphology']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Related Words:**  ['united states', 'earth', 'water', 'latin', 'glacier', 'new york city']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Most Frequent Grams:**  ['mr. horenstein', 'new york', 'horenstein said', 'york city', 'ice', 'ridge']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Most Important Sentences:**  ['At the start of the last ice age, 2.6 million years ago, a sheet of frozen water formed atop North America that kept expanding and thickening until it reached a maximum depth of roughly two miles.', 'The intermittent ridge runs from Puget Sound to the Missouri River to Montauk Point on Long Island, forming the prominence that supports its old lighthouse.', 'The ice over Manhattan would have buried even the tallest skyscraper and was so heavy that it depressed the underlying bedrock.']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(\"**Topic Model**\")\n",
    "printmd(\"**General Categories:**  {}\".format(important_categories))\n",
    "printmd(\"**Related Words:**  {}\".format(important_links))\n",
    "printmd(\"**Most Frequent Grams:**  {}\".format(important_grams))\n",
    "printmd(\"**Most Important Sentences:**  {}\".format(important_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#After this step, emotion data will be downloaded and verbs will be analyzed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formed VERB VBN acl sheet\n",
      "\n",
      "kept VERB VBD relcl sheet\n",
      "\n",
      "expanding VERB VBG xcomp kept\n",
      "\n",
      "reached VERB VBD advcl kept\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "doc = nlp('At the start of the last ice age, 2.6 million years ago, a sheet of frozen water formed atop North America that kept expanding and thickening until it reached a maximum depth of roughly two miles.', 'The intermittent ridge runs from Puget Sound to the Missouri River to Montauk Point on Long Island, forming the prominence that supports its old lighthouse.')\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_=='VERB':\n",
    "        print(token, token.pos_, token.tag_, token.dep_, token.head)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
